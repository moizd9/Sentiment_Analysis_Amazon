# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SDL6jTEC2BurKKou-jT9CW6tY5-Z5nS_
"""

!pip install pandas matplotlib seaborn nltk wordcloud textblob

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
from textblob import TextBlob
import string
from collections import Counter

amazon_reviews = pd.read_csv('/content/amazon_reviews.csv')

amazon_reviews.head(10)

"""# **Treating the reviewText Column**

### Converting texts into lower case

Let's begin with standardizing the texts in the reviewText column by converting all the characters into the lower case. This helps the data look uniform.
"""

amazon_reviews['reviewText'] = amazon_reviews['reviewText'].str.lower()
amazon_reviews['reviewText']

"""### Treating Punctuations"""

amazon_reviews['reviewText'] = df['reviewText'].str.translate(str.maketrans('', '', string.punctuation))

amazon_reviews['reviewText']

"""### Removing numbers from the comments"""

amazon_reviews['reviewText'] = amazon_reviews['reviewText'].str.replace(r'\d+', '', regex=True)

amazon_reviews['reviewText']

"""### Removing Stopwords

Why? - becuase stop words are like "doesn't, don't, hasn't" etc which is better to be removed as it reduces the noise and increases the focus of the algorithm on other important words like "terrible", "bad", "excellent", etc.
"""

nltk.download('stopwords')

#English stopwords set
stop_words = set(stopwords.words('english'))

# Removing stopwords
amazon_reviews['reviewText'] = amazon_reviews['reviewText'].apply(lambda x: " ".join(x for x in str(x).split() if x not in stop_words))

amazon_reviews['reviewText']

"""### Removing Rare Words

WHY? We remove rare words (i.e. words that appear only once or very few times across all reviews) because this helps clean up noise and reduce dimensionality, especially useful in modeling and visualization.

- First we will count the frequence of all the words and then will remove those who are used only once.
"""

#flattening all words in the corpus and count them
word_freq = Counter(" ".join(amazon_reviews['reviewText']).split())

#checking rare words (e.g., words that appear only once)
rare_words = set([word for word, freq in word_freq.items() if freq == 1])

#now i will remove rare words from each review
amazon_reviews['reviewText'] = amazon_reviews['reviewText'].apply(
    lambda x: ' '.join([word for word in x.split() if word not in rare_words])
)

"""### Tokenization

creating TextBlob objects and tokenizing reviews into words is a great next step for further NLP tasks like lemmatization
"""

!python -m textblob.download_corpora

#Creating a new column with TextBlob objects
amazon_reviews['blob'] = amazon_reviews['reviewText'].apply(lambda x: TextBlob(x))

#Tokenizing the reviews into words
amazon_reviews['tokens'] = amazon_reviews['blob'].apply(lambda x: x.words)
amazon_reviews[['reviewText', 'tokens']].head()

#Lemmatizing each word using TextBlob
amazon_reviews['lemmatized'] = amazon_reviews['blob'].apply(
    lambda x: ' '.join([word.lemmatize() for word in x.words])
)


amazon_reviews[['reviewText', 'lemmatized']].head()

"""# **Visualization**

### FIrst we will check the frequency of words from Lemmitized comments
"""

#Combine all lemmatized reviews into one string
all_words = ' '.join(amazon_reviews['lemmatized']).split()

#word frequencies
word_freq = Counter(all_words)

#making intto DataFrame
tf_df = pd.DataFrame(word_freq.items(), columns=['word', 'tf'])

#Sorting by term frequency
tf_df = tf_df.sort_values(by='tf', ascending=False).reset_index(drop=True)

tf_df.head(10)

"""### Bar Plot"""

#number of top terms to visualize is 20
top_n = 20
top_tf = tf_df.head(top_n)

plt.figure(figsize=(12, 6))
sns.barplot(data=top_tf, x='tf', y='word', palette='viridis')
plt.title(f"Top {top_n} Most Frequent Words in Amazon Reviews")
plt.xlabel("Term Frequency")
plt.ylabel("Words")
plt.tight_layout()
plt.show()

"""### Wordcloud"""

#here i will combine all lemmatized text into one string
text = ' '.join(amazon_reviews['lemmatized'])

wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(text)
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of Amazon Review Terms", fontsize=18)
plt.show()

"""## **Top Insights**

**Dominant Words:**

- "phone", "card", "gb", "use", and "one" appear the most.

- These likely indicate that the product is a memory card or storage device for phones, given how commonly "gb", "phone", and "card" are mentioned together.

**Brand Mentions:**

- Words like "sandisk", "samsung", and "amazon" appear, highlighting popular brands referenced by customers.

**Use Cases:**

- Frequent use of "camera", "tablet", "computer", and "file" suggests people are using these products across different digital devices.

- Terms like "picture", "movie", "music", and "storage" reveal how customers are leveraging the product.

**Customer Sentiment Cues:**

- Positive descriptors like "great", "good", "fast", "reliable", and "perfectly" indicate overall satisfaction.

- Occasional mentions of "problem", "issue", and "complaint" suggest some negative experiences, but they are smaller and less frequent.

## **What This Tells Us**

- The reviews are likely about microSD cards or phone storage solutions.

- Customers focus on capacity (e.g., "gb"), device compatibility, and performance (e.g., "speed", "work").

- Most reviews reflect positive user experiences with brand-specific mentions (especially SanDisk and Samsung).

# Sentiment Analysis
"""

# i will check polarity score for each review
amazon_reviews['polarity'] = amazon_reviews['lemmatized'].apply(lambda x: TextBlob(x).sentiment.polarity)

#now assigning sentiment category based on polarity
def get_sentiment(p):
    if p > 0:
        return 'Positive'
    elif p < 0:
        return 'Negative'
    else:
        return 'Neutral'

amazon_reviews['sentiment'] = amazon_reviews['polarity'].apply(get_sentiment)

amazon_reviews[['lemmatized', 'polarity', 'sentiment']].head()

"""### Sentiments distribution on a bar plot"""

plt.figure(figsize=(7,5))
sns.countplot(data=amazon_reviews, x='sentiment', palette='pastel', order=['Positive', 'Neutral', 'Negative'])
plt.title('Sentiment Distribution of Amazon Reviews')
plt.xlabel('Sentiment Category')
plt.ylabel('Number of Reviews')
plt.show()

"""### now I am comparing sentiment vs. overall star ratings that helps validate whether text sentiment aligns with numeric ratings (e.g., 1â€“5 stars)."""

# Ensure 'overall' is numeric
amazon_reviews['overall'] = pd.to_numeric(amazon_reviews['overall'], errors='coerce')

# Cross-tab: how sentiment is distributed across star ratings
sentiment_rating_ct = pd.crosstab(amazon_reviews['overall'], amazon_reviews['sentiment'])
sentiment_rating_ct

# Plotting sentiment counts per star rating
sentiment_rating_ct.plot(kind='bar', figsize=(10,6), colormap='Set3')
plt.title('Sentiment Distribution by Star Rating')
plt.xlabel('Star Rating')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

"""# Machine Learning Modelling"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""### Preparing Features"""

#my Features and target
X = amazon_reviews['lemmatized']
y = amazon_reviews['sentiment']

#Spliting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""### TF-IDF Vectorization"""

tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

"""### Training Logistic Regression Model"""

model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train_tfidf, y_train)

"""### Evaluate Model Performance"""

#Predictions
y_pred = model.predict(X_test_tfidf)

#Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Instantiate model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, class_weight='balanced')

# Train
rf_model.fit(X_train_tfidf, y_train)

# Predict
y_pred_rf = rf_model.predict(X_test_tfidf)

# Evaluate
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

"""As we can see that both of our models have a good overall accuracy.
However, what I think is that both models are still underperforming when it comes to representing the minority class (the Negatives). Therefore I will now use the SMOTE method to see if I can improve on this part.

### SMOTE (Synthetic Minority Over-sampling Technique)
"""

!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE

#applying SMOTE on training data only
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_tfidf, y_train)

#seeing new class distribution
from collections import Counter
print("Before SMOTE:", Counter(y_train))
print("After SMOTE:", Counter(y_train_balanced))

"""### Training Logistic Regression on the balanced data"""

model_smote = LogisticRegression(max_iter=1000)
model_smote.fit(X_train_balanced, y_train_balanced)

#Evaluating this now
y_pred_smote = model_smote.predict(X_test_tfidf)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("Accuracy:", accuracy_score(y_test, y_pred_smote))
print("\nClassification Report:\n", classification_report(y_test, y_pred_smote))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_smote))

"""# Model Comparison Summary

Below is a comparison of the three models i applied for sentiment classification using TF-IDF vectors on Amazon reviews.

#### **1. Logistic Regression with class_weight='balanced'**
I used this model to address class imbalance without changing the dataset by adjusting weights during training.

- Accuracy: 81.6%
- Negative recall: 0.70
- Neutral recall: 0.64
- Positive recall: 0.84
- Macro F1-score: 0.64

This model performed best for the negative class but slightly struggled on neutral reviews. It is simple, efficient, and easy to interpret.

#### **2. Random Forest with class_weight='balanced'**
I used this ensemble model to capture more complex patterns and interactions.

- Accuracy: 85.2%
- Negative recall: 0.25
- Neutral recall: 0.80
- Positive recall: 0.92
- Macro F1-score: 0.66

While it achieved the highest overall accuracy, it underperformed on detecting negative reviews and may be prone to overfitting with sparse data.

#### **3. Logistic Regression with SMOTE oversampling**
I used this to synthetically balance the dataset by oversampling minority classes.

- Accuracy: 84.5%
- Negative recall: 0.65
- Neutral recall: 0.66
- Positive recall: 0.88
- Macro F1-score: 0.67

This model provided the most balanced performance across all sentiment classes, making it the most reliable for fair sentiment prediction.


"""